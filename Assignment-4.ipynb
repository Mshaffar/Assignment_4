{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Exercise 1\n",
    "\n",
    "Daskalopoulos Ioannis (f3351805)<br>\n",
    "Ntouskas Fotios (f3351813)<br>\n",
    "Palassopoulos Vasileios (f3351814)<br>\n",
    "Spantouri Natalia (f3351817)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csc_matrix, vstack\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import sklearn.exceptions\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.layers import InputSpec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Bidirectional, Input\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To load fasttext pre-trained embeddings more efficiently,\n",
    "we can read only once the embeddings file and save:\n",
    "1/ A 2D np-array for the embedding vectors \n",
    "2/ A dictionary that maps each word to the row index of its\n",
    "    embedding on the 2D np-array\n",
    "'''\n",
    "\n",
    "idx = 0\n",
    "vocab = {}\n",
    "with open(\"cc.en.300.vec\", 'r', encoding=\"utf-8\", newline='\\n',errors='ignore') as f:\n",
    "    for l in f:\n",
    "        line = l.rstrip().split(' ')\n",
    "        if idx == 0:\n",
    "            vocab_size = int(line[0]) + 1\n",
    "            dim = int(line[1])\n",
    "            vecs = np.zeros(vocab_size*dim).reshape(vocab_size,dim)\n",
    "            vocab[\"__PADDING__\"] = 0\n",
    "            idx = 1\n",
    "        else:\n",
    "            vocab[line[0]] = idx\n",
    "            emb = np.array(line[1:]).astype(np.float)\n",
    "            if (emb.shape[0] == dim):\n",
    "                vecs[idx,:] = emb\n",
    "                idx+=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    pickle.dump(vocab,open(\"fasttext_voc\",'wb'))\n",
    "    np.save(\"fasttext.npy\",vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings np-array and mapping dictionary\n",
    "fasttext_embed = np.load(\"fasttext.npy\")\n",
    "fasttext_word_to_index = pickle.load(open(\"fasttext_voc\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Emails Recursively by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get root folder, Extensions that will be read, Class Label\n",
    "def read_data(directory,file_extension,class_label):\n",
    "    \n",
    "    data = []\n",
    "    index = []\n",
    "    #Recursively read file paths starting from root directory, ending with the specisfied extension, 2 levels deep\n",
    "    for filename in glob.glob(directory+'/**/*'+file_extension, recursive=True):\n",
    "        email_content = \"\"                                                   #This is our email string\n",
    "        with open(filename, 'r',encoding='utf-8', errors='ignore') as email: #Open the file in the located file path\n",
    "            for line in email:                        #Read the email line by line and replace the tabs and new lines with space\n",
    "                line = line.replace('\\n', ' ')\n",
    "                line = line.replace('\\t', ' ')\n",
    "                email_content+= line                               #Append the line to the email string\n",
    "        email.close()                                              #Close the file\n",
    "        data.append({'text': email_content, 'label': class_label}) #Append a dictionary with email string and class label\n",
    "        index.append(filename)                                     #Append the path\n",
    "                    \n",
    "    return data,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\1306.2004-05-29.GP.spam.txt</th>\n",
       "      <td>Spam</td>\n",
       "      <td>Subject: re : are you still online ? click here to be removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\0347.2004-02-05.GP.spam.txt</th>\n",
       "      <td>Spam</td>\n",
       "      <td>Subject: hi if you are paying more than 3 . 6 % on your mortgage , we can save you money ! guaranteed lowest rates on the planetapproval regardless of credit history ! start saving todayshow me the lowest rates to stop receiving offers here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron6\\spam\\2646.2005-01-11.BG.spam.txt</th>\n",
       "      <td>Spam</td>\n",
       "      <td>Subject: urgent hi , i hereby wish to inform you that i am interested to purchase your , pci cardteac cd - w 54 e cd - r / rw burner , sony sdt - 5000 dds 2 ( 4 / 8 gb ) dat drive , sony 15 cl dds dat cleaning tape . what is your best offer ? are you the real owner ? what is the condition ? payment will be by money order . dont worried yourself about the pick up . i will take care of that when payment is done . if its still available for sale , provide the details below with which the payment will be sent to . . . offeri look forward to read from you soonest . cheers _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ do you yahoo ! ? tired of spam ? yahoo ! mail has the best spam protection around http : / / mail . yahoo . com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           label  \\\n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\1306.2004-05-29.GP.spam.txt  Spam   \n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\0347.2004-02-05.GP.spam.txt  Spam   \n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron6\\spam\\2646.2005-01-11.BG.spam.txt  Spam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\1306.2004-05-29.GP.spam.txt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Subject: re : are you still online ? click here to be removed   \n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron4\\spam\\0347.2004-02-05.GP.spam.txt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Subject: hi if you are paying more than 3 . 6 % on your mortgage , we can save you money ! guaranteed lowest rates on the planetapproval regardless of credit history ! start saving todayshow me the lowest rates to stop receiving offers here   \n",
       "C:\\Users\\Fotis\\Desktop\\Enron_dataset\\Enron_dataset\\enron6\\spam\\2646.2005-01-11.BG.spam.txt  Subject: urgent hi , i hereby wish to inform you that i am interested to purchase your , pci cardteac cd - w 54 e cd - r / rw burner , sony sdt - 5000 dds 2 ( 4 / 8 gb ) dat drive , sony 15 cl dds dat cleaning tape . what is your best offer ? are you the real owner ? what is the condition ? payment will be by money order . dont worried yourself about the pick up . i will take care of that when payment is done . if its still available for sale , provide the details below with which the payment will be sent to . . . offeri look forward to read from you soonest . cheers _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ do you yahoo ! ? tired of spam ? yahoo ! mail has the best spam protection around http : / / mail . yahoo . com  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enron_dataset_dir = os.getcwd() + \"\\\\Enron_dataset\" #Path\n",
    "Ham_data,index_hamdata = read_data(Enron_dataset_dir,\".ham.txt\",\"Ham\")           #Create array with Ham emails\n",
    "Spam_data,index_spamdata = read_data(Enron_dataset_dir,\".spam.txt\",\"Spam\")       #Create array with Spam emails\n",
    "data = pd.DataFrame(Ham_data,index =index_hamdata)                               #Make them a dataframe with their path as the index\n",
    "data = data.append(pd.DataFrame(Spam_data,index =index_spamdata))                #Append the spam data\n",
    "#shuffle dataframe      \n",
    "data = shuffle(data,random_state = 456987)                                       \n",
    "pd.set_option(\"max_colwidth\",2000)                                               #Set max columns to not overload Jupyter\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam    17171\n",
       "Ham     16545\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Test, Held-out and Train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, heldout, test  = np.split(data, [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(itertools.chain.from_iterable(train[['text']].values.tolist())) #Extract x_train from the train set\n",
    "x_heldout = list(itertools.chain.from_iterable(heldout[['text']].values.tolist())) #Extract x_heldout from the heldout set\n",
    "x_test = list(itertools.chain.from_iterable(test[['text']].values.tolist()))   #Extract x_test from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(itertools.chain.from_iterable(train[['label']].values.tolist())) #Extract training labels\n",
    "le = preprocessing.LabelEncoder()                                               #Initialize Labelencoder\n",
    "le.fit(y_train)                                                                 \n",
    "y_train = le.transform(y_train)                                                 #Y_train classes become 0 and 1\n",
    "y_heldout = le.transform(list(itertools.chain.from_iterable(heldout[['label']].values.tolist())))\n",
    "y_test = le.transform(list(itertools.chain.from_iterable(test[['label']].values.tolist()))) #Same for y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions for recall, precision, f1 and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert texts to sequence of indexes and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS =20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_seqs = tokenizer.texts_to_sequences(x_train)\n",
    "heldout_seqs = tokenizer.texts_to_sequences(x_heldout)\n",
    "test_seqs = tokenizer.texts_to_sequences(x_test)\n",
    "train_data = pad_sequences(train_seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "heldout_data = pad_sequences(heldout_seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_seqs, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124106 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model's embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_WORDS:\n",
    "            continue\n",
    "    try:\n",
    "        embedding_vector = fasttext_embed[fasttext_word_to_index[word],:]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom keras layer for linear attention over RNNs output states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average attention mechanism \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.w = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_w'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.w]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, h, mask=None):\n",
    "        h_shape = K.shape(h)\n",
    "        d_w, T = h_shape[0], h_shape[1]\n",
    "        \n",
    "        logits = K.dot(h, self.w)  # w^T h\n",
    "        logits = K.reshape(logits, (d_w, T))\n",
    "        alpha = K.exp(logits - K.max(logits, axis=-1, keepdims=True))  # exp\n",
    "        \n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            alpha = alpha * mask\n",
    "        alpha = alpha / K.sum(alpha, axis=1, keepdims=True) # softmax\n",
    "        r = K.sum(h * K.expand_dims(alpha), axis=1)  # r = h*alpha^T\n",
    "        h_star = K.tanh(r)  # h^* = tanh(r)\n",
    "        if self.return_attention:\n",
    "            return [h_star, alpha]\n",
    "        return h_star\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV RNN + linear attention model with an MLP on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.972871 using {'number_RNNs': 1, 'method': <class 'keras.layers.recurrent.GRU'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 8, 'dropout': 0.0, 'batch_size': 64, 'SIZE': 200}\n",
      "0.941035 (0.002137) with: {'number_RNNs': 2, 'method': <class 'keras.layers.recurrent.GRU'>, 'input_layer_units': 256, 'hidden_layers': 1, 'epochs': 2, 'dropout': 0.0, 'batch_size': 32, 'SIZE': 300}\n",
      "0.968523 (0.006974) with: {'number_RNNs': 4, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 14, 'dropout': 0.0, 'batch_size': 32, 'SIZE': 100}\n",
      "0.972871 (0.000388) with: {'number_RNNs': 1, 'method': <class 'keras.layers.recurrent.GRU'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 8, 'dropout': 0.0, 'batch_size': 64, 'SIZE': 200}\n",
      "0.963189 (0.000548) with: {'number_RNNs': 1, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 4, 'dropout': 0.0, 'batch_size': 32, 'SIZE': 300}\n",
      "0.968061 (0.001551) with: {'number_RNNs': 2, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 8, 'dropout': 0.375, 'batch_size': 32, 'SIZE': 100}\n",
      "0.891483 (0.077995) with: {'number_RNNs': 4, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 12, 'dropout': 0.0, 'batch_size': 32, 'SIZE': 200}\n",
      "0.944220 (0.014996) with: {'number_RNNs': 4, 'method': <class 'keras.layers.recurrent.GRU'>, 'input_layer_units': 512, 'hidden_layers': 1, 'epochs': 4, 'dropout': 0.125, 'batch_size': 32, 'SIZE': 200}\n",
      "0.958817 (0.000805) with: {'number_RNNs': 1, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 256, 'hidden_layers': 1, 'epochs': 12, 'dropout': 0.5, 'batch_size': 64, 'SIZE': 200}\n",
      "0.965153 (0.003418) with: {'number_RNNs': 1, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 256, 'hidden_layers': 2, 'epochs': 8, 'dropout': 0.0, 'batch_size': 32, 'SIZE': 100}\n",
      "0.966817 (0.006204) with: {'number_RNNs': 2, 'method': <class 'keras.layers.recurrent.LSTM'>, 'input_layer_units': 256, 'hidden_layers': 1, 'epochs': 8, 'dropout': 0.125, 'batch_size': 64, 'SIZE': 200}\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_layer_units=512, hidden_layers=1, dropout=0.5, optimizer='adam', SIZE = 100, method=GRU, number_RNNs=1):\n",
    "    \n",
    "    inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "    embeddings = Embedding(MAX_WORDS+1,EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "    drop_emb = Dropout(dropout)(embeddings)\n",
    "    \n",
    "    for k in range(number_RNNs):\n",
    "        if k == 0:\n",
    "            bilstm = Bidirectional(method(units=SIZE, return_sequences=True))(drop_emb)\n",
    "        else:\n",
    "            bilstm = Bidirectional(method(units=SIZE, return_sequences=True))(bilstm)\n",
    "            \n",
    "    x, attn = AttentionWeightedAverage(return_attention=True)(bilstm)\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        if i == 0:\n",
    "            out = Dense(units=input_layer_units, activation=\"relu\")(x)\n",
    "        else:\n",
    "            input_layer_units = input_layer_units//2\n",
    "            out = Dense(units=input_layer_units, activation=\"relu\")(out)\n",
    "        out = Dropout(dropout)(out)\n",
    "    \n",
    "    out = Dense(units=1, activation=\"sigmoid\")(out)\n",
    "    model = Model(inputs, out)\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(lr=0.001),\n",
    "                      metrics=[f1])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "hyperparameters = {\n",
    "    'method': [LSTM,GRU],\n",
    "    'SIZE': [100,200,300],\n",
    "    'number_RNNs':  np.arange(1,5,1),\n",
    "    'input_layer_units': [256, 512],\n",
    "    'hidden_layers' : np.arange(1,3,1),\n",
    "    'batch_size' : [32, 64],\n",
    "    'dropout' : np.linspace(0,0.5,5),\n",
    "    'epochs': np.arange(0,15,2)\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(model, hyperparameters, n_iter = 10, scoring='f1', cv=2, verbose=0, n_jobs=1)\n",
    "best_model = rnd_search.fit(heldout_data,y_heldout)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (best_model.best_score_, best_model.best_params_))\n",
    "means = best_model.cv_results_['mean_test_score']\n",
    "stds = best_model.cv_results_['std_test_score']\n",
    "params = best_model.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending the heldout set on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=vstack((train_data, heldout_data)).tocsr()\n",
    "y_train=np.concatenate([y_train, y_heldout],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train a Bi-LSTM + linear attention model with an MLP on top of it using optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 1000, 300)         6000300   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 1000, 400)         601200    \n",
      "_________________________________________________________________\n",
      "attention_weighted_average_1 [(None, 400), (None, 1000 400       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 6,807,725\n",
      "Trainable params: 807,425\n",
      "Non-trainable params: 6,000,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e9b51834014093a24683b64a77f3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=8, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.97069, saving model to keras_BiLSTM+attn_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.97069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_f1 improved from 0.97069 to 0.98080, saving model to keras_BiLSTM+attn_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_f1 improved from 0.98080 to 0.98280, saving model to keras_BiLSTM+attn_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.98280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_f1 improved from 0.98280 to 0.98504, saving model to keras_BiLSTM+attn_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.98504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=26972, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.98504\n"
     ]
    }
   ],
   "source": [
    "GRU_SIZE = 200\n",
    "DENSE = 512\n",
    "N_CLASSES = 1\n",
    "\n",
    "inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "embeddings = Embedding(MAX_WORDS+1,EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "drop_emb = Dropout(0)(embeddings)\n",
    "bilstm = Bidirectional(GRU(units=GRU_SIZE, return_sequences=True))(drop_emb)\n",
    "x, attn = AttentionWeightedAverage(return_attention=True)(bilstm)\n",
    "out = Dense(units=DENSE, activation=\"relu\")(x)\n",
    "out = Dense(units=N_CLASSES, activation=\"sigmoid\")(out)\n",
    "model = Model(inputs, out)\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "checkpoint = ModelCheckpoint('keras_BiLSTM+attn_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "train_performance=model.fit(train_data, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=8,\n",
    "              verbose = 0,\n",
    "              callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "              validation_data=(test_data, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 1000, 300)         6000300   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 1000, 400)         601200    \n",
      "_________________________________________________________________\n",
      "attention_weighted_average_1 [(None, 400), (None, 1000 400       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 6,807,725\n",
      "Trainable params: 807,425\n",
      "Non-trainable params: 6,000,300\n",
      "_________________________________________________________________\n",
      "\n",
      "Test Binary_cross_entropy: 0.0507\n",
      "\n",
      "Test precision: 0.9807\n",
      "\n",
      "Test recall: 0.9899\n",
      "\n",
      "Test f1: 0.9850\n",
      "\n",
      "Test accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "GRU_SIZE = 200\n",
    "DENSE = 512\n",
    "N_CLASSES = 1\n",
    "\n",
    "inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "embeddings = Embedding(MAX_WORDS+1,EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "drop_emb = Dropout(0)(embeddings)\n",
    "bilstm = Bidirectional(GRU(units=GRU_SIZE, return_sequences=True))(drop_emb)\n",
    "x, attn = AttentionWeightedAverage(return_attention=True)(bilstm)\n",
    "out = Dense(units=DENSE, activation=\"relu\")(x)\n",
    "out = Dense(units=N_CLASSES, activation=\"sigmoid\")(out)\n",
    "modelev = Model(inputs, out)\n",
    "\n",
    "print(modelev.summary())\n",
    "\n",
    "modelev.load_weights(\"keras_BiLSTM+attn_model\")\n",
    "modelev.compile(loss='binary_crossentropy', optimizer='adam', metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "score = modelev.evaluate(\n",
    "    test_data,\n",
    "    y_test,          \n",
    "    batch_size=64,\n",
    "    verbose=1)\n",
    "\n",
    "print('\\nTest Binary_cross_entropy: %.4f' %  (score[0]))\n",
    "print('\\nTest precision: %.4f' %  (score[1]))\n",
    "print('\\nTest recall: %.4f' %  (score[2]))\n",
    "print('\\nTest f1: %.4f' % (score[3]))\n",
    "print('\\nTest accuracy: %.4f'% (score[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves - Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "   \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title('Learning Curves (%s)' % (title))\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, scoring='f1', train_sizes=train_sizes, verbose=0)\n",
    "    \n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"b\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"orange\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"orange\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "    DummyClassifier(strategy='most_frequent'), X, y, cv=cv, n_jobs=n_jobs, scoring='f1', train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Baseline score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"green\", \n",
    "             label=\"Cross-validation Baseline score\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    return plt\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "plot_learning_curve(model, 'MLP Classifier', train_data, y_train, ylim=(0.5, 1.01), cv=cv, n_jobs=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='ignore')\n",
    "    \n",
    "pred = model.predict(test_data)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, pred)\n",
    "area = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(recall, precision, label='Model Precision-Recall curve')\n",
    "\n",
    "pred = DummyClassifier(strategy='most_frequent').fit(train_data,y_train).predict(test_data)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, pred)\n",
    "\n",
    "plt.plot(recall, precision, label='Baseline Precision-Recall curve')\n",
    "plt.grid()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall %s: AUC=%0.2f' % ('MLP Classifier',area))\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the misclassified results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_from_dataset(index_pos):\n",
    "    print('True Email Class: ' + test.iloc[index_pos]['label'])\n",
    "    print(test.iloc[index_pos]['text'][:2000] + '....')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array = modelev.predict(test_data).reshape((-1,))\n",
    "predict_classes_array = modelev.predict_classes(test_data).reshape((-1,))\n",
    "real_classes_array = y_test\n",
    "predict_array[predict_classes_array == 0] = 1 - predict_array[predict_classes_array == 0]\n",
    "predict_array[predict_classes_array == real_classes_array] = 0\n",
    "misclassifications = np.argsort(predict_array)[::-1]\n",
    "\n",
    "for index, _ in zip(misclassifications, range(20)):\n",
    "    get_email_from_dataset(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
